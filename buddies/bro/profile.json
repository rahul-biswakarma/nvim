{
  "llm": {
    "model": "llama3",
    "temperature": 0.6,
    "max_tokens": 10000
  }
}
